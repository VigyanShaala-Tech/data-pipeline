{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3163ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required modules\n",
    "import pandas as pd\n",
    "import mysql.connector as msql\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d69552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying all columns and rows\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c6648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from She for STEM - Uttarakhand.csv:\n"
     ]
    }
   ],
   "source": [
    "#Reading She for STEM Incubator file present on source files\n",
    "directory_path =(r\"C:\\Users\\spjay\\OneDrive - VigyanShaala\\02 Products  Initiatives\\01 SheForSTEM\\05 Kalpana M&E\\00 DBMS 1.0\\Kalpana\\Kalpana\\04 Incubator - Curriculum Cohort\\Incubator 6.0 UK\\data_files\\02_Source_Kalpana Incubator\")\n",
    "csv_files = [file for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory_path, file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(f\"Data from {file}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb7912e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of column names to extract for putting into database \n",
    "columns_to_extract = ['Name', 'Email', 'Segment', 'Mobile', 'Enroll Date', 'Assigned Through', 'Course Name','Start Date','Last Login']\n",
    "\n",
    "# Use the loc method to extract the specified columns\n",
    "df = data.loc[:, columns_to_extract]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947e066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with 'email' column and columns that start with 'Week' 'Video', 'Recording', and 'Master class'\n",
    "data = data[['Email'] + [col for col in data.columns if col.startswith(('Week','Video', 'SUK', 'Workshop', 'Master Class'))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb40371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the columns of data dataframe in oldcol\n",
    "oldcol = list(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dfb38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oldcol =data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14caa5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Email',\n",
       " 'Week 0 : Overview - Program Kalpana - 6pgx1c40pq',\n",
       " 'Video ! : Why women in STEM. What has Kalpana program achieved until now? - 62c7d78d0cf273488220b8d4',\n",
       " 'Video @ : What do we expect from our Kalpana fellows? - 62c7d10e0cf273488220b5bf',\n",
       " 'SUK + : Program Inauguration & Icebreaker 7 Sept - 66dc3e605ae308684ff5e726',\n",
       " 'Week 1 : Setting Aspirational Goals - ng1lgwrxx4',\n",
       " 'Video # : Goal setting - Part 1 - 6529ffc5e4b0a564951cb119',\n",
       " 'Video $ : Goal setting - Part 2 - 6529ffd4e4b08a0cfb8d0628',\n",
       " 'Workshop @ : Setting Aspirational Goals, Thursday, Sept 12, 3-4 PM - 66de9bf79027bf0127e793ec',\n",
       " 'Master Class # : Searching and Securing Internships - Friday, Sept 13, 3-4 PM - 66e125d35e916d19ae3df9b1',\n",
       " 'Week 2 : Reflect on Setting Goals - b1jsvbvfh1',\n",
       " 'SUK ! : Speak Up Kalpana session, Friday, Sept 20, 3-4 PM - 66e13e6e91e40e365df6fdd4',\n",
       " 'Week 3 : Setting SMART goals - 19n1ky6p5n',\n",
       " 'Video % : What is SMART goal ? - 62b3fded0cf2f5120a60c932',\n",
       " 'Video ^ : What is the difference between a goal and a SMART goal ? - 62b3fe320cf2f5120a60c946',\n",
       " 'Workshop $ : Setting SMART Goals, Thursday, Sept 26, 3-4 PM - 66e13f3476aec348317d2f82',\n",
       " 'Master Class $ : Acing your CV and Resume - Friday, Sept 27, 3-4 PM - 669bcf052dc20b47fbf2fe99',\n",
       " 'Week 4 : Acing your Resume - ivw86avdks',\n",
       " 'SUK @ : Speak Up Kalpana session, Friday, Oct 04, 3-4 PM - 66e16961350fd25781854113',\n",
       " 'Workshop !! : Resume Building, Thursday, Oct 03, 3-4 PM - 66fe60ed848a814842ca4817',\n",
       " 'Week 5: Career Action Planner - Goals/Strategy videos - m92obhzh4v',\n",
       " 'Video $? : What is Career Action Planner (CAP)? - 66adf521860b71150ba38ee7',\n",
       " 'Video %+ : CAP -Setting aspirational goals - 66adf537860b71150ba38ef0',\n",
       " 'Video %! : CAP - Common dangers of goal setting - 66adf54c323eaf7ca90260c2',\n",
       " 'Workshop !# : Resume Part 2 - Tuesday, Oct 08, 3-4 PM - 66e1292a0b2ff80834043181',\n",
       " 'Workshop % : CAP - Goal A and Goal B, Thursday, Oct 10, 3-4 PM - 66e157bb9764e21dc82fefe1',\n",
       " 'Master Class ! : Career Exploration -Friday, Oct 11, 3-4 PM - 66e16af647cfba543f5bfc02',\n",
       " 'Week 6 : Career Exploration - dlf671j9pl',\n",
       " 'Workshop ^ : Career Exploration, Thursday, Oct 17, 3-4 PM - 66e1590a07ac5d223c16704a',\n",
       " 'SUK # : Speak Up Kalpana session, Friday, Oct 18, 3-4 PM - 66e16a2a7786466b28e20d98',\n",
       " 'Week 7 : CAP Careers/Skills - c5f68rat2n',\n",
       " 'Video %@ : CAP - Career exploration (Finding key employers, career options and skills to learn) - 66ca11aabdcd457fa70393bf',\n",
       " 'Workshop & : CAP - Careers, Thursday, Oct 24, 3-4 PM - 66e15985ed86491af380d907',\n",
       " 'Week 8 : Holiday Break - lg48n1tvaa',\n",
       " 'Week 9 : CAP - Setting Milestones - 8ii7erpfpx',\n",
       " 'Video %# : Importance of milestones - Setting milestones & Adjustments - 66d1cce5e7eb841494dbdcbc',\n",
       " 'Video %$ : CAP Overview & How to use after Kalpana - 66d1cdd362a6586f5d9d8487',\n",
       " 'Workshop * : CAP - Milestones, Thursday, Nov 07, 3-4 PM - 672f58e966985879c05686cd',\n",
       " 'Master Class % : Making the most of LinkedIn - Friday, Nov 08, 3-4 PM - 6726f73b9b5cf20d990b91ce',\n",
       " 'Week 10: SWOT Analysis - 2z1ndjomfw',\n",
       " 'Video & : What is SWOT & its importance - 65dd86dbe4b06c493da95be5',\n",
       " 'Video * : How to do SWOT analysis? - 65dd86f7e4b0b6b63afd1fa1',\n",
       " 'Video ? : What are Strengths, Weakness, Opportunities, Threats - 65dd86ede4b05b3f96b721a8',\n",
       " 'Video !+ : Deep dive into SWOT template with example - 62c56bfa0cf2d3022e87e6c5',\n",
       " 'Workshop ? : SWOT, Thursday, Nov 14, 3-4 PM - 66e15c6c589f9621d8a97795',\n",
       " 'Master Class !^ : Job Interview Preparation, Friday, Nov 15, 3-4 PM - 66e16c0a47cfba543f5bfd06',\n",
       " 'Workshop !@ : Securing Internships & LinkedIn, Nov 21, 3-4 PM - 66e15ea30e23e702f79f9df6',\n",
       " 'SUK $ : Program Finale - 66e16db64147c71f5908246d']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c0d1325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503, 48)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheaking shape of our dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed3e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to keep track of the current week, video count, recording count, and master class count\n",
    "week_col = None\n",
    "video_count = 0\n",
    "recording_count = 0\n",
    "master_count = 0\n",
    "workshop_count = 0\n",
    "\n",
    "# Create an empty list to store the new column names\n",
    "new_cols = []\n",
    "\n",
    "# Iterate over each column in the data\n",
    "for col in data.columns:\n",
    "    # If the column starts with 'Week'\n",
    "    if col.startswith('Week'):\n",
    "        # Split the column name by space and get the second element (the week number)\n",
    "        week_col = col.split()[1]\n",
    "        # Reset the video, recording, and master class counts for the new week\n",
    "        video_count = 0\n",
    "        recording_count = 0\n",
    "    # If the column starts with 'Video'\n",
    "    elif col.startswith('Video'):\n",
    "        # Increment the video count for the current week\n",
    "        video_count += 1\n",
    "        # Append a new column name to the list using f-string formatting\n",
    "        new_cols.append(f'WK{week_col}_V{video_count}')\n",
    "    # If the column starts with 'Workshop'\n",
    "    elif col.startswith('Workshop'):\n",
    "        # Increment the master class count for the current week\n",
    "        workshop_count += 1\n",
    "        # Append a new column name to the list using f-string formatting\n",
    "        new_cols.append(f'WK{week_col}_WS{workshop_count}')\n",
    "    # If the column doesn't start with any of the above prefixes\n",
    "    # If the column starts with 'Recording'\n",
    "    elif col.startswith('SUK'):\n",
    "        # Increment the recording count for the current week\n",
    "        recording_count += 1\n",
    "        # Append a new column name to the list using f-string formatting\n",
    "        new_cols.append(f'WK{week_col}_SUK_V')\n",
    "    # If the column starts with 'Master Class'\n",
    "    elif col.startswith('Master Class'):\n",
    "        # Increment the master class count for the current week\n",
    "        master_count += 1\n",
    "        # Append a new column name to the list using f-string formatting\n",
    "        new_cols.append(f'WK{week_col}_Master{master_count}')\n",
    "    # If the column doesn't start with any of the above prefixes\n",
    "    else:\n",
    "        # Append the original column name to the list\n",
    "        new_cols.append(col)\n",
    "\n",
    "# Remove all columns that start with 'Week' from the data\n",
    "data = data.loc[:, ~data.columns.str.startswith('Week')]\n",
    "# Assign the new column names to the data\n",
    "data.columns = new_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a0f79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503, 37)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rechecking the shape of dataframe\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57433c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'WK0_V1', 'WK0_V2', 'WK0_SUK_V', 'WK1_V1', 'WK1_V2', 'WK1_WS1',\n",
       "       'WK1_Master1', 'WK2_SUK_V', 'WK3_V1', 'WK3_V2', 'WK3_WS2',\n",
       "       'WK3_Master2', 'WK4_SUK_V', 'WK4_WS3', 'WK5:_V1', 'WK5:_V2', 'WK5:_V3',\n",
       "       'WK5:_WS4', 'WK5:_WS5', 'WK5:_Master3', 'WK6_WS6', 'WK6_SUK_V',\n",
       "       'WK7_V1', 'WK7_WS7', 'WK9_V1', 'WK9_V2', 'WK9_WS8', 'WK9_Master4',\n",
       "       'WK10:_V1', 'WK10:_V2', 'WK10:_V3', 'WK10:_V4', 'WK10:_WS9',\n",
       "       'WK10:_Master5', 'WK10:_WS10', 'WK10:_SUK_V'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheaking columns name\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6e75356-593e-42d0-9fe5-93e46fbae18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename duplicate column names correctly\n",
    "def rename_duplicate_columns(data):\n",
    "    seen = {}\n",
    "    new_columns = []\n",
    "    \n",
    "    for col in data.columns:\n",
    "        if col not in seen:\n",
    "            seen[col] = 1\n",
    "            new_columns.append(col)\n",
    "        else:\n",
    "            seen[col] += 1\n",
    "            new_columns.append(f\"{col}{seen[col]}\")\n",
    "    \n",
    "    data.columns = new_columns\n",
    "    return data\n",
    "\n",
    "# Applying the function\n",
    "data = rename_duplicate_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a347a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns starting with 'WKNone' to 'WK0' and removing : if present\n",
    "data.columns = [\n",
    "    col.replace('WKNone', 'WK0').replace(':', '') if col.startswith('WKNone') or ':' in col else col\n",
    "    for col in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e200734-bd5b-462b-8c67-0ab95b8e639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.rename(columns = {'WK10_SUK_V': 'WK10_SUK_V1'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4387bf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheaking for duplicate data\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "644b4cda-d01c-489e-b224-9a14ee8d3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating columns for extracting only numbers\n",
    "col1 = data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f767acc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WK0_V1           object\n",
       "WK0_V2           object\n",
       "WK0_SUK_V        object\n",
       "WK1_V1           object\n",
       "WK1_V2           object\n",
       "WK1_WS1          object\n",
       "WK1_Master1      object\n",
       "WK2_SUK_V        object\n",
       "WK3_V1           object\n",
       "WK3_V2           object\n",
       "WK3_WS2          object\n",
       "WK3_Master2      object\n",
       "WK4_SUK_V        object\n",
       "WK4_WS3          object\n",
       "WK5_V1           object\n",
       "WK5_V2           object\n",
       "WK5_V3           object\n",
       "WK5_WS4          object\n",
       "WK5_WS5          object\n",
       "WK5_Master3      object\n",
       "WK6_WS6          object\n",
       "WK6_SUK_V        object\n",
       "WK7_V1           object\n",
       "WK7_WS7          object\n",
       "WK9_V1           object\n",
       "WK9_V2           object\n",
       "WK9_WS8          object\n",
       "WK9_Master4      object\n",
       "WK10_V1          object\n",
       "WK10_V2          object\n",
       "WK10_V3          object\n",
       "WK10_V4          object\n",
       "WK10_WS9         object\n",
       "WK10_Master5     object\n",
       "WK10_WS10       float64\n",
       "WK10_SUK_V      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33820cd-1da4-4b24-9a4a-d63e1416f245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a785de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for extracting only numbers from dataset\n",
    "for column in [i for i in col1.columns if col1[i].dtype == 'object']:\n",
    "    data[column] = data[column].astype(str).str.extract('(\\d+)').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aff28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling Null values with zero\n",
    "fillna = data.iloc[:,1:]\n",
    "fillnacol=fillna.columns\n",
    "data[fillnacol]=data[fillnacol].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f27646ad-785e-4324-816b-ffb38366e267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'WK0_V1', 'WK0_V2', 'WK0_SUK_V', 'WK1_V1', 'WK1_V2', 'WK1_WS1',\n",
       "       'WK1_Master1', 'WK2_SUK_V', 'WK3_V1', 'WK3_V2', 'WK3_WS2',\n",
       "       'WK3_Master2', 'WK4_SUK_V', 'WK4_WS3', 'WK5_V1', 'WK5_V2', 'WK5_V3',\n",
       "       'WK5_WS4', 'WK5_WS5', 'WK5_Master3', 'WK6_WS6', 'WK6_SUK_V', 'WK7_V1',\n",
       "       'WK7_WS7', 'WK9_V1', 'WK9_V2', 'WK9_WS8', 'WK9_Master4', 'WK10_V1',\n",
       "       'WK10_V2', 'WK10_V3', 'WK10_V4', 'WK10_WS9', 'WK10_Master5',\n",
       "       'WK10_WS10', 'WK10_SUK_V'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5bb410-19c3-42d0-97a9-c3bb55c0caec",
   "metadata": {},
   "source": [
    "## Merging data from Live Session Attendance Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a78a2f-7ecc-4c8f-b80d-dcd67ec0bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Google Sheets document\n",
    "\n",
    "url = \"\" # Enter Google Sheet Link here\n",
    "\n",
    "# The URL needs to be modified to point to the export format\n",
    "\n",
    "url = url.replace('/edit?usp=sharing', '/export?format=xlsx')\n",
    "\n",
    "# Read the data from the URL into a pandas DataFrame\n",
    "\n",
    "la = pd.read_excel(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e2ac3a7-5d52-463b-b41b-9be03a7df89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns to keep\n",
    "columns_to_keep = [col for col in la.columns if col.startswith(\"Timestamp\") or col.startswith(\"Enter the email ID that you use to login to the VigyanShaala app/platform\") ]\n",
    "\n",
    "# Drop the columns not in columns_to_keep\n",
    "la.drop(columns=[col for col in la.columns if col not in columns_to_keep], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff96738c-0701-477a-be0e-52cb5d0db18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Enter the email ID that you use to login to the VigyanShaala app/platform' in la.columns: la.rename(columns={'Enter the email ID that you use to login to the VigyanShaala app/platform': 'Email'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6e18497-54b7-4e4d-a5e0-d57f5583a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Timestamp column to datetime\n",
    "la['Timestamp'] = pd.to_datetime(la['Timestamp'])\n",
    "\n",
    "# Create a new column 'dates' with the desired format\n",
    "la['Date'] = la['Timestamp'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "142562f7-df91-488d-ba10-dae1b5f99bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Timestamp column\n",
    "la = la.drop(columns=['Timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2affda7-b4a9-4381-abd4-cee4e98d8e9e",
   "metadata": {},
   "source": [
    "###  👇 The date and the Name of session should be same so it will get merge 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a94a1c6f-e7b3-479a-8fa2-060a8c24bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Excel file form our source\n",
    "ld  = pd.read_excel(r\"C:\\Users\\spjay\\OneDrive - VigyanShaala\\02 Products  Initiatives\\01 SheForSTEM\\05 Kalpana M&E\\00 DBMS 1.0\\Kalpana\\Kalpana\\04 Incubator - Curriculum Cohort\\Incubator 6.0 UK\\data_files\\00_Source_Kalpana Live Session Info\\Live Session Dates.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3e6ea02-c5c7-4dd9-953b-27339e37a3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Sessions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-09-07</td>\n",
       "      <td>WK0_SUK_V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-12</td>\n",
       "      <td>WK1_WS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-09-13</td>\n",
       "      <td>WK1_Master1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-09-20</td>\n",
       "      <td>WK2_SUK_V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-09-26</td>\n",
       "      <td>WK3_WS2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date     Sessions\n",
       "0 2024-09-07    WK0_SUK_V\n",
       "1 2024-09-12      WK1_WS1\n",
       "2 2024-09-13  WK1_Master1\n",
       "3 2024-09-20    WK2_SUK_V\n",
       "4 2024-09-26      WK3_WS2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61f4a356-9634-48b2-bb39-4343c1e4dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime format\n",
    "la['Date'] = pd.to_datetime(la['Date'])\n",
    "ld['Date'] = pd.to_datetime(ld['Date'])\n",
    "\n",
    "# Merge dataframes on 'Date'\n",
    "md = pd.merge(la, ld, on='Date', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "503506f5-7421-4d23-b1d7-1be3dad22c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Email' column has null values\n",
    "md = md.dropna(subset=['Email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3faee22-6b70-4310-89c4-8083c92bc1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'Date', 'Sessions'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dec40edc-3424-4280-a6a7-5526cc5ebb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean email addresses\n",
    "def clean_email(email):\n",
    "\n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    cleaned_email = email.lower().strip()\n",
    "    # Remove patterns like \".com.1\"\n",
    "    cleaned_email = re.sub(r'\\.com\\.\\d+', '.com', cleaned_email)\n",
    "    return cleaned_email\n",
    "\n",
    "# Apply the function to the 'email' column\n",
    "md['Email'] = md['Email'].apply(clean_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a8a9a6f-de23-43ed-b132-9fe7a88201e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'name' column\n",
    "md = md.drop(columns=['Date'])\n",
    "\n",
    "# Add a new column 'duration' with all values set to 3600\n",
    "md['Duration'] = 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d7801aa-0c82-4ab1-b66a-e61315c965a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_pivot = md.pivot_table(index='Email', columns='Sessions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "023dd188-7c8e-430d-991c-b171ce701391",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_pivot.columns = [f'{col[0]} - {col[1]}' for col in md_pivot.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42c14bc9-41ee-46dd-923e-8c0b4b33be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab740c17-1b54-4056-98ce-ad8d0b2ff708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns of dataframe\n",
    "md_pivot.columns = md_pivot.columns.str.replace(r'Duration - ', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06582c91-6e6b-4f00-a88d-29b15ba50718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes on 'Email' column\n",
    "merged_df = pd.merge(data, md_pivot, on='Email', how='outer', suffixes=('_df', '_da'))\n",
    "\n",
    "# List of columns to sum\n",
    "columns_to_sum = [col for col in md_pivot.columns if col != 'Email']\n",
    "\n",
    "# Sum the matching columns\n",
    "for col in columns_to_sum:\n",
    "    merged_df[col] = merged_df[f'{col}_df'].fillna(0) + merged_df[f'{col}_da'].fillna(0)\n",
    "\n",
    "# Drop the intermediate columns\n",
    "merged_df.drop(columns=[f'{col}_df' for col in columns_to_sum] + [f'{col}_da' for col in columns_to_sum], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb14d16-2a9e-4124-a551-e5f119e4405e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6bf94dc-dc3e-4312-bb4a-1f6baf3bee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that merged_df contains all columns from gd\n",
    "missing_cols = [col for col in data.columns if col not in merged_df.columns]\n",
    "for col in missing_cols:\n",
    "    merged_df[col] = pd.NA\n",
    "\n",
    "# Reorder the columns of merged_df to match the order of gd.columns\n",
    "merged_df = merged_df[data.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10feb962",
   "metadata": {},
   "source": [
    "# Enroll Date MySQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29288023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new table which for Enroll_Dates which is taken from incubator graphy sheet\n",
    "Enroll=pd.DataFrame(df[[\"Email\", 'Enroll Date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eee1cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only Enroll_Date\n",
    "Enroll[['Enroll Date','Time']]=Enroll['Enroll Date'].str.split(' ',expand=True)\n",
    "Enroll=Enroll.drop([\"Time\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266cbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to MySQL Database\n",
    "conn= msql.connect(host='',user='',password=\"@123\",database=\"\",auth_plugin='')\n",
    "cursor =conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98b34cd1-4631-449d-be55-7c1ed8936448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inserting data to 03_enroll_date table\n",
    "for i,row in Enroll.iterrows():\n",
    "    cursor.execute(\"insert IGNORE into 03_enroll_date (Email,Incubator) values(%s,%s)\",tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e76184db-b015-4208-857a-74393dd3a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing data  with new data to 03_enroll_date table\n",
    "for i,row in Enroll.iterrows():\n",
    "    cursor.execute(\"REPLACE into 03_enroll_date (Email,incubator) values(%s,%s)\",tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4449f431-7951-4671-af10-a5787f65e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a887fd3",
   "metadata": {},
   "source": [
    "# Payment details MySQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e33fd7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new table which for Payment which is taken from incubator graphy sheet\n",
    "Payment=pd.DataFrame(df[[\"Email\",\"Assigned Through\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7eb101-4bdd-4449-bc9b-bbc0bae4872c",
   "metadata": {},
   "source": [
    "# Extracting only Payment removing Order Id\n",
    "Payment[[\"Assigned Through\",\"Order_ID\"]]=Payment[\"Assigned Through\"].str.split(\"-\",expand=True)\n",
    "Payment.drop([\"Order_ID\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2958182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning fee cost to different cathegory of enrollment\n",
    "Payment[\"Assigned Through\"]=Payment[\"Assigned Through\"].replace(['Admin'],'0')\n",
    "Payment[\"Assigned Through\"]=Payment[\"Assigned Through\"].replace(['Excel Upload'],'0')\n",
    "Payment[\"Assigned Through\"]=Payment[\"Assigned Through\"].replace(['Paid Transaction '],'1899')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to MySQL Database\n",
    "conn= msql.connect(host='',user='',password=\"\",database=\"\",auth_plugin='')\n",
    "cursor =conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39eb5a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inserting data into 02_payment_details \n",
    "for i,row in Payment.iterrows():\n",
    "    cursor.execute(\"insert Ignore into 02_payment_details (Email,Incubator_Fee) values(%s,%s)\",tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56a1a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing data with new data into 02_payment_details \n",
    "for i,row in Payment.iterrows():\n",
    "    cursor.execute(\"REPLACE into 02_payment_details (Email,Incubator_Fee) values(%s,%s)\",tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c69c9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49039773-7a46-438c-9ef7-8f5c015454b3",
   "metadata": {},
   "source": [
    "# Batch Lastlogin Startdate MySQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "afb6da61-e49e-4563-8e70-2a74fa46369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new table which for Enroll_Dates which is taken from incubator graphy sheet\n",
    "Batch=pd.DataFrame(df[[\"Email\", 'Last Login','Start Date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70487f8f-13ff-418a-9021-fc8e76aec128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only Enroll_Date\n",
    "Batch[['Last Login','Time']]=Batch['Last Login'].str.split(' ',expand=True)\n",
    "Batch=Batch.drop([\"Time\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b2990e0-88d4-4941-8f7f-465169351db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert 'Start Date' format\n",
    "def convert_start_date(date_str):\n",
    "    if re.match(r'\\d{4}/\\d{2}/\\d{2}', date_str):\n",
    "        return pd.to_datetime(date_str, format='%Y/%m/%d').strftime('%d-%m-%Y')\n",
    "    return date_str\n",
    "\n",
    "# Function to convert 'Last Login' format\n",
    "def convert_last_login(date_str):\n",
    "    if re.match(r'\\d{2}/\\d{2}/\\d{2}', date_str):\n",
    "        return pd.to_datetime(date_str, format='%d/%m/%y').strftime('%d-%m-%Y')\n",
    "    return date_str\n",
    "\n",
    "\n",
    "# Apply the conversion functions to the respective columns\n",
    "Batch['Start Date'] = Batch['Start Date'].apply(convert_start_date)\n",
    "Batch['Last Login'] = Batch['Last Login'].apply(convert_last_login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9e0a4-d58f-42ea-ae71-96de042cd826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to MySQL Database\n",
    "conn= msql.connect(host='',user='',password=\"\",database=\"\",auth_plugin='')\n",
    "cursor =conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e723111-e817-42d3-be7b-8100df1d16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting data to 15_batch_lastlogin_startdate table\n",
    "for i,row in Batch.iterrows():\n",
    "    cursor.execute(\"insert IGNORE into 15_batch_lastlogin_startdate (Email,Incubator_Last_Login,Incubator_Start_Date) values(%s,%s,%s)\",tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8a695cc-ddca-4d36-81b9-c3c13f7e0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing data  with new data to 03_enroll_date table\n",
    "for i,row in Batch.iterrows():\n",
    "    cursor.execute(\"REPLACE into 15_batch_lastlogin_startdate (Email,Incubator_Last_Login,Incubator_Start_Date) values(%s,%s,%s)\",tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "130c92dc-5298-4bcd-b6ca-16d331f03c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17e19c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Changing seconds to Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ed22662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving previous column name \n",
    "newcol = list(merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bbbf5a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Email',\n",
       " 'WK0_V1',\n",
       " 'WK0_V2',\n",
       " 'WK0_SUK_V',\n",
       " 'WK1_V1',\n",
       " 'WK1_V2',\n",
       " 'WK1_WS1',\n",
       " 'WK1_Master1',\n",
       " 'WK2_SUK_V',\n",
       " 'WK3_V1',\n",
       " 'WK3_V2',\n",
       " 'WK3_WS2',\n",
       " 'WK3_Master2',\n",
       " 'WK4_SUK_V',\n",
       " 'WK4_WS3',\n",
       " 'WK5_V1',\n",
       " 'WK5_V2',\n",
       " 'WK5_V3',\n",
       " 'WK5_WS4',\n",
       " 'WK5_WS5',\n",
       " 'WK5_Master3',\n",
       " 'WK6_WS6',\n",
       " 'WK6_SUK_V',\n",
       " 'WK7_V1',\n",
       " 'WK7_WS7',\n",
       " 'WK9_V1',\n",
       " 'WK9_V2',\n",
       " 'WK9_WS8',\n",
       " 'WK9_Master4',\n",
       " 'WK10_V1',\n",
       " 'WK10_V2',\n",
       " 'WK10_V3',\n",
       " 'WK10_V4',\n",
       " 'WK10_WS9',\n",
       " 'WK10_Master5',\n",
       " 'WK10_WS10',\n",
       " 'WK10_SUK_V']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0553f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving previous column name for data1 by removing \"Email column\"\n",
    "merged_df_columns = list(merged_df.columns)\n",
    "\n",
    "# Exclude the 'email' column if it exists\n",
    "if 'Email' in merged_df_columns:\n",
    "    merged_df_columns.remove('Email')\n",
    "\n",
    "merged_df1col = merged_df_columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01ef9b3f-6376-4da9-9995-34bd5285edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns starting with \"Week\" from the list of old columns\n",
    "oldcol = [col for col in oldcol if not col.startswith(\"Week\")]\n",
    "\n",
    "# Determine the number of columns to rename (minimum of available columns)\n",
    "num_columns_to_rename = min(len(merged_df.columns), len(oldcol))\n",
    "\n",
    "# Rename columns using a dictionary mapping\n",
    "merged_df.rename(columns=dict(zip(merged_df.columns[:num_columns_to_rename], oldcol[:num_columns_to_rename])), inplace=True)\n",
    "\n",
    "# Remove any additional information after the colon in column names\n",
    "merged_df.columns = merged_df.columns.str.split(':').str[0]\n",
    "\n",
    "# Remove leading and trailing spaces from column names\n",
    "merged_df.columns = merged_df.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db56390d-e5dc-477e-b4d5-45dd7ce03c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'Video !', 'Video @', 'SUK +', 'Video #', 'Video $',\n",
       "       'Workshop @', 'Master Class #', 'SUK !', 'Video %', 'Video ^',\n",
       "       'Workshop $', 'Master Class $', 'SUK @', 'Workshop !!', 'Video $?',\n",
       "       'Video %+', 'Video %!', 'Workshop !#', 'Workshop %', 'Master Class !',\n",
       "       'Workshop ^', 'SUK #', 'Video %@', 'Workshop &', 'Video %#', 'Video %$',\n",
       "       'Workshop *', 'Master Class %', 'Video &', 'Video *', 'Video ?',\n",
       "       'Video !+', 'Workshop ?', 'Master Class !^', 'Workshop !@', 'SUK $'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fcee9688-e966-470e-bef9-49d7e367f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(col):\n",
    "    # Split the column name by space\n",
    "    parts = col.split()\n",
    "    special_chars = {'!': '1', '@': '2', '#': '3', '$': '4', '%': '5', '^': '6', '&': '7', '*': '8', '?': '9', '+': '0'}\n",
    "    \n",
    "    # If the first part is 'SUK', add the number corresponding to the last character\n",
    "    if parts[0] == 'SUK':\n",
    "        return parts[0] + special_chars.get(parts[1][-1], '0')\n",
    "    \n",
    "    # If the first part is 'Video', use the prefix 'VID' and the numbers corresponding to the last two characters\n",
    "    elif parts[0] == 'Video':\n",
    "        last_char = parts[1][-1]\n",
    "        second_last_char = parts[1][-2] if len(parts[1]) > 1 else '+'\n",
    "        return 'VID' + special_chars.get(second_last_char, '0') + special_chars.get(last_char, '0')\n",
    "\n",
    "    # If the first part is 'Workshop', use the prefix 'VID' and the numbers corresponding to the last two characters\n",
    "    elif parts[0] == 'Workshop':\n",
    "        last_char = parts[1][-1]\n",
    "        second_last_char = parts[1][-2] if len(parts[1]) > 1 else '+'\n",
    "        return 'WS' + special_chars.get(second_last_char, '0') + special_chars.get(last_char, '0')\n",
    "    \n",
    "    # If the first part contains 'Master' and 'Class' is present, use the prefix 'MC' and the numbers corresponding to the last two characters\n",
    "    elif 'Master' in parts and 'Class' in parts:\n",
    "        index = parts.index('Master')\n",
    "        last_char = parts[index + 2][-1]\n",
    "        second_last_char = parts[index + 2][-2] if len(parts[index + 2]) > 1 else '+'\n",
    "        return 'MC' + special_chars.get(second_last_char, '0') + special_chars.get(last_char, '0')\n",
    "    \n",
    "    # Otherwise, return the column name as it is\n",
    "    else:\n",
    "        return col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8714917b-1c86-4e67-bab6-a62954c0280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column names using the rename_column function\n",
    "merged_df.rename(columns=rename_column, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8362edd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'VID01', 'VID02', 'SUK0', 'VID03', 'VID04', 'WS02', 'MC03',\n",
       "       'SUK1', 'VID05', 'VID06', 'WS04', 'MC04', 'SUK2', 'WS11', 'VID49',\n",
       "       'VID50', 'VID51', 'WS13', 'WS05', 'MC01', 'WS06', 'SUK3', 'VID52',\n",
       "       'WS07', 'VID53', 'VID54', 'WS08', 'MC05', 'VID07', 'VID08', 'VID09',\n",
       "       'VID10', 'WS09', 'MC16', 'WS12', 'SUK4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a978424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Excel file form our source\n",
    "excel_file  = pd.read_excel(r\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "29430c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df1=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c753d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Email' present in 'merged_df' but not in the Excel file. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each column in the 'data' DataFrame\n",
    "for column_name in merged_df.columns:\n",
    "    # Check if the column exists in the Excel file\n",
    "    if column_name in excel_file['Code'].values:\n",
    "        # Get the corresponding value in the \"Time\" column from the Excel file\n",
    "        value = excel_file.loc[excel_file['Code'] == column_name, 'Time'].values[0]\n",
    "        \n",
    "        # Calculate the percentage value\n",
    "        percentage_value = (merged_df[column_name] * 100) / value\n",
    "        \n",
    "        # Assign the calculated percentage value to a new column in the \"merged_df1\" DataFrame with the same name as the \"Code\" column\n",
    "        merged_df1[column_name] = percentage_value\n",
    "    else:\n",
    "        # Handle the case when the column is present in 'merged_df' but not in the Excel file\n",
    "        print(f\"Column '{column_name}' present in 'merged_df' but not in the Excel file. Skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8e259cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the Excel file\n",
    "for index, row in excel_file.iterrows():\n",
    "    # Get the value in the \"Code\" column\n",
    "    column_name = row['Code']\n",
    "    \n",
    "    # Get the value in the \"Time\" column\n",
    "    value = row['Time']\n",
    "    \n",
    "    # Check if the 'Code' column exists in the 'merged_df' DataFrame\n",
    "    if column_name in merged_df.columns:\n",
    "        # Calculate the percentage value\n",
    "        percentage_value = (merged_df[column_name] * 100) / value\n",
    "        \n",
    "        # Assign the calculated percentage value to a new column in the \"merged_df1\" DataFrame with the same name as the \"Code\" column\n",
    "        merged_df1[column_name] = percentage_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c19289c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing column name back to new column\n",
    "merged_df.columns = newcol \n",
    "merged_df1.columns = merged_df1col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bd7b5ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'WK0_V1', 'WK0_V2', 'WK0_SUK_V', 'WK1_V1', 'WK1_V2', 'WK1_WS1',\n",
       "       'WK1_Master1', 'WK2_SUK_V', 'WK3_V1', 'WK3_V2', 'WK3_WS2',\n",
       "       'WK3_Master2', 'WK4_SUK_V', 'WK4_WS3', 'WK5_V1', 'WK5_V2', 'WK5_V3',\n",
       "       'WK5_WS4', 'WK5_WS5', 'WK5_Master3', 'WK6_WS6', 'WK6_SUK_V', 'WK7_V1',\n",
       "       'WK7_WS7', 'WK9_V1', 'WK9_V2', 'WK9_WS8', 'WK9_Master4', 'WK10_V1',\n",
       "       'WK10_V2', 'WK10_V3', 'WK10_V4', 'WK10_WS9', 'WK10_Master5',\n",
       "       'WK10_WS10', 'WK10_SUK_V'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "66ce69ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WK0_V1', 'WK0_V2', 'WK0_SUK_V', 'WK1_V1', 'WK1_V2', 'WK1_WS1',\n",
       "       'WK1_Master1', 'WK2_SUK_V', 'WK3_V1', 'WK3_V2', 'WK3_WS2',\n",
       "       'WK3_Master2', 'WK4_SUK_V', 'WK4_WS3', 'WK5_V1', 'WK5_V2', 'WK5_V3',\n",
       "       'WK5_WS4', 'WK5_WS5', 'WK5_Master3', 'WK6_WS6', 'WK6_SUK_V', 'WK7_V1',\n",
       "       'WK7_WS7', 'WK9_V1', 'WK9_V2', 'WK9_WS8', 'WK9_Master4', 'WK10_V1',\n",
       "       'WK10_V2', 'WK10_V3', 'WK10_V4', 'WK10_WS9', 'WK10_Master5',\n",
       "       'WK10_WS10', 'WK10_SUK_V'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "de1ed33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function \"Max_Value\" for converting % max 100 if values are above 100\n",
    "def Max_Value(value):\n",
    "    if value >=100:\n",
    "        return 100\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7bf6adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spjay\\AppData\\Local\\Temp\\ipykernel_44632\\3614295319.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  merged_df1[per1c]=merged_df1[per1c].applymap(Max_Value)\n"
     ]
    }
   ],
   "source": [
    "# Applying Max_Value function\n",
    "per1=merged_df1.iloc[:,0:]\n",
    "per1c=per1.columns\n",
    "merged_df1[per1c]=merged_df1[per1c].applymap(Max_Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa7b1a",
   "metadata": {},
   "source": [
    "# Code for converting seconds to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b941d2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "# Creating a function for converting seconds to hours\n",
    "def Convert_Hours(seconds):\n",
    "    hours = seconds / (3600)\n",
    "    return hours\n",
    "n = 5400\n",
    "print(Convert_Hours(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0975e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Convert_Hours function\n",
    "col1 = merged_df.iloc[:,1:]\n",
    "col=col1.columns\n",
    "merged_df[col]=merged_df[col].apply(Convert_Hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3744605c",
   "metadata": {},
   "source": [
    "# Code for Pre Recorded Videos: Total hours, Percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d38d5a7",
   "metadata": {},
   "source": [
    "# 👇 Input Weeks Here 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3047c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the week number up to which you want to include columns\n",
    "end_week = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3a9db805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all column names up to the end week\n",
    "cols_to_select = []\n",
    "for week in range(end_week + 1):\n",
    "    for video in range(1, 10):\n",
    "        col_name = f'WK{week}_V{video}'\n",
    "        if col_name in merged_df.columns:\n",
    "            cols_to_select.append(col_name)\n",
    "\n",
    "# Select the desired columns and create a new DataFrame\n",
    "Recorded_Total = merged_df[cols_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2955953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"Pre_Recorded_Total_Hours\"]=Recorded_Total.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a6267a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all column names up to the end week\n",
    "cols_to_select_per = []\n",
    "for week in range(end_week + 1):\n",
    "    for video in range(1, 10):\n",
    "        col_name = f'WK{week}_V{video}'\n",
    "        if col_name in merged_df1.columns:\n",
    "            cols_to_select_per.append(col_name)\n",
    "\n",
    "# Select the desired columns and create a new DataFrame\n",
    "Recorded_Percent = merged_df1[cols_to_select_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "12e886f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"Pre_Recorded_Percentage\"]=Recorded_Percent.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f68a5",
   "metadata": {},
   "source": [
    "# Code for SUK Videos: Total hours & Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a84e6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all column names up to the end week\n",
    "cols_to_select = []\n",
    "for week in range(end_week + 1):\n",
    "    col_name = f'WK{week}_SUK_V'\n",
    "    if col_name in merged_df.columns:\n",
    "        cols_to_select.append(col_name)\n",
    "\n",
    "# Select the desired columns and create a new DataFrame\n",
    "SUK_Recorded_Total = merged_df[cols_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ee3cc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"SUK_Total_Hours\"]=SUK_Recorded_Total.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "67bcb0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all column names up to the end week\n",
    "cols_to_select = []\n",
    "for week in range(end_week + 1):\n",
    "    col_name = f'WK{week}_SUK_V'\n",
    "    if col_name in merged_df1.columns:\n",
    "        cols_to_select.append(col_name)\n",
    "\n",
    "# Select the desired columns and create a new DataFrame\n",
    "SUK_Recorded_Percent = merged_df1[cols_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0e8f2769-41b6-4e02-84ff-1e321d51276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spjay\\AppData\\Local\\Temp\\ipykernel_44632\\4141890018.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SUK_Recorded_Percent.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "SUK_Recorded_Percent.fillna(0, inplace=True)\n",
    "# Remove columns with all zero values\n",
    "SUK_Recorded_Percent = SUK_Recorded_Percent.loc[:, (SUK_Recorded_Percent != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e1d40f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean, excluding columns that contain all zeros\n",
    "merged_df[\"SUK_Percentage\"]=SUK_Recorded_Percent.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20013b8e",
   "metadata": {},
   "source": [
    "# Code for Master Class: Total hours & Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ac9a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all column names up to the end week\n",
    "cols_to_select = []\n",
    "for week in range(end_week + 2):\n",
    "    for masterclass in range(1, 50):\n",
    "        col_name = f'WK{week}_Master{masterclass}'\n",
    "        if col_name in merged_df.columns:\n",
    "            cols_to_select.append(col_name)\n",
    "\n",
    "# Select the desired columns and create a new DataFrame\n",
    "Masterclass_Total = merged_df[cols_to_select]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e17d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"Masterclass_Total_Hours\"]=Masterclass_Total.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0aac8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all column names up to the end week\n",
    "cols_to_select = []\n",
    "for week in range(end_week + 1):\n",
    "    for masterclass in range(1, 10):\n",
    "        col_name = f'WK{week}_Master{masterclass}'\n",
    "        if col_name in merged_df1.columns:\n",
    "            cols_to_select.append(col_name)\n",
    "\n",
    "# Select the desired columns and create a new DataFrame\n",
    "Masterclass_Percent = merged_df1[cols_to_select]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3b534fe8-a985-4187-ab52-f707c414a11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spjay\\AppData\\Local\\Temp\\ipykernel_44632\\1041955373.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Masterclass_Percent.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "Masterclass_Percent.fillna(0, inplace=True)\n",
    "# Remove columns with all zero values\n",
    "Masterclass_Percent = Masterclass_Percent.loc[:, (Masterclass_Percent != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6cf36241-7e31-476d-8ec3-2955c801c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and assign it to the new column\n",
    "merged_df[\"Masterclass_Percentage\"] = Masterclass_Percent.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4c533-13be-49ef-b8b1-4b18cef501c8",
   "metadata": {},
   "source": [
    "# Code for Workshop Class: Total hours & Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5dec8b18-518e-406f-ac1f-00547e1fb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all column names up to the end week\n",
    "cols_to_select = []\n",
    "for week in range(end_week + 1):\n",
    "    for video in range(1, 10):\n",
    "        col_name = f'WK{week}_WS{video}'\n",
    "        if col_name in merged_df.columns:\n",
    "            cols_to_select.append(col_name)\n",
    "\n",
    "# Select the desired columns and create a new DataFrame\n",
    "WS_Total = merged_df[cols_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1a312fd8-6a18-40b0-92d3-f4d097f3dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"Workshop_Total_Hours\"]=WS_Total.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9daa7f69-1dbb-41e8-9420-0341875cdd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all column names up to the end week\n",
    "cols_to_select_per = []\n",
    "for week in range(end_week + 1):\n",
    "    for video in range(1, 10):\n",
    "        col_name = f'WK{week}_WS{video}'\n",
    "        if col_name in merged_df1.columns:\n",
    "            cols_to_select_per.append(col_name)\n",
    "\n",
    "# Select the desired columns and create a new DataFrame\n",
    "WS_Percent = merged_df1[cols_to_select_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c543beac-1dc9-45f9-9dc6-7740fc8568c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spjay\\AppData\\Local\\Temp\\ipykernel_44632\\2660094332.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  WS_Percent.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "WS_Percent.fillna(0, inplace=True)\n",
    "# Remove columns with all zero values\n",
    "WS_Percent = WS_Percent.loc[:, (WS_Percent != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cf86bb9d-3ba1-4c81-b48b-1819c0ecf314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean, excluding columns that contain all zeros\n",
    "merged_df[\"Workshop_Percentage\"]=WS_Percent.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63929f07",
   "metadata": {},
   "source": [
    "# Code for Program: Total hours & Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "99cebd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns from data where the column names start with \"WK\"\n",
    "selected_cols = [col for col in merged_df.columns if col.startswith('WK')]\n",
    "\n",
    "# create a new dataframe called Whole_Program_Total with the selected columns\n",
    "Whole_Program_Total = merged_df[selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "94c11f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"Program_Total_Hours\"]=Whole_Program_Total.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "66baeb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns from data where the column names start with \"WK\"\n",
    "selected_cols_1 = [col for col in merged_df.columns if col.startswith(\"Pre_Recorded_Percentage\") or col.startswith(\"SUK_Percentage\") or col.startswith(\"Masterclass_Percentage\") or col.startswith(\"Workshop_Percentage\")]\n",
    "\n",
    "# create a new dataframe called Whole_Program_Total with the selected columns\n",
    "Whole_Program_Percent = merged_df[selected_cols_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a23c8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"Program_Percentage\"]=Whole_Program_Percent.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a5eab004",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=merged_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b70fc5cd-13bf-4715-ba84-328e2c1ea04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean email addresses\n",
    "def clean_email(email):\n",
    "\n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    cleaned_email = email.lower().strip()\n",
    "    # Remove patterns like \".com.1\"\n",
    "    cleaned_email = re.sub(r'\\.com\\.\\d+', '.com', cleaned_email)\n",
    "    return cleaned_email\n",
    "\n",
    "# Apply the function to the 'email' column\n",
    "merged_df['Email'] = merged_df['Email'].apply(clean_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4fcd8ff1-935c-4ec7-a54e-48779b45cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rearrange columns\n",
    "def rearrange_columns(merged_df):\n",
    "    def extract_week_number(col):\n",
    "        match = re.search(r'WK(\\d+)', col)\n",
    "        return int(match.group(1)) if match else -1\n",
    "\n",
    "    wk_columns = sorted([col for col in merged_df.columns if col.startswith('WK')], key=extract_week_number)\n",
    "    total_columns = [col for col in merged_df.columns if col.endswith('Hours')]\n",
    "    percentage_columns = [col for col in merged_df.columns if col.endswith('Percentage')]\n",
    "    other_columns = [col for col in merged_df.columns if col not in wk_columns + total_columns + percentage_columns]\n",
    "    \n",
    "    new_column_order = other_columns + wk_columns + total_columns + percentage_columns\n",
    "    return merged_df[new_column_order]\n",
    "\n",
    "# Applying the function\n",
    "merged_df = rearrange_columns(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bfb39-b674-4ba3-8ea3-2bfcf9934bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5ee03d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean email addresses\n",
    "def clean_email(email):\n",
    "\n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    cleaned_email = email.lower().strip()\n",
    "    # Remove patterns like \".com.1\"\n",
    "    cleaned_email = re.sub(r'\\.com\\.\\d+', '.com', cleaned_email)\n",
    "    return cleaned_email\n",
    "\n",
    "# Apply the function to the 'email' column\n",
    "merged_df['Email'] = merged_df['Email'].apply(clean_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d293a-005f-4fd0-8b3f-6a9e6736d891",
   "metadata": {},
   "source": [
    "# Adding Genral Info and Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26d3c5-085a-46f6-8264-9d12d4292ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data from Genral Info\n",
    "gi = pd.read_csv(r\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "52895f7c-19bf-49ce-8ae8-49b0334a803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking columns we want\n",
    "gi = gi[['Email', 'Name', 'Phone', 'Name_of_College_University','Currently_Pursuing_Degree']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b61086f7-979b-4507-97e5-69f89abf1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with gi\n",
    "output = pd.merge(gi, merged_df, on='Email', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500c96c-82fe-40aa-985d-f1452159913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting data to Output files\n",
    "output.to_csv(r\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d19f3a",
   "metadata": {},
   "source": [
    "# Incubator and attendance monitoring MySQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "25b25aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'WK0_V1', 'WK0_V2', 'WK0_SUK_V', 'WK1_V1', 'WK1_V2', 'WK1_WS1',\n",
       "       'WK1_Master1', 'WK2_SUK_V', 'WK3_V1', 'WK3_V2', 'WK3_WS2',\n",
       "       'WK3_Master2', 'WK4_SUK_V', 'WK4_WS3', 'WK5_V1', 'WK5_V2', 'WK5_V3',\n",
       "       'WK5_WS4', 'WK5_WS5', 'WK5_Master3', 'WK6_WS6', 'WK6_SUK_V', 'WK7_V1',\n",
       "       'WK7_WS7', 'WK9_V1', 'WK9_V2', 'WK9_WS8', 'WK9_Master4', 'WK10_V1',\n",
       "       'WK10_V2', 'WK10_V3', 'WK10_V4', 'WK10_WS9', 'WK10_Master5',\n",
       "       'WK10_WS10', 'WK10_SUK_V', 'Pre_Recorded_Total_Hours',\n",
       "       'SUK_Total_Hours', 'Masterclass_Total_Hours', 'Workshop_Total_Hours',\n",
       "       'Program_Total_Hours', 'Pre_Recorded_Percentage', 'SUK_Percentage',\n",
       "       'Masterclass_Percentage', 'Workshop_Percentage', 'Program_Percentage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6c38894b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'WK0_V1', 'WK0_V2', 'WK0_SUK_V', 'WK1_V1', 'WK1_V2', 'WK1_WS1',\n",
       "       'WK1_Master1', 'WK2_SUK_V', 'WK3_V1', 'WK3_V2', 'WK3_WS2',\n",
       "       'WK3_Master2', 'WK4_SUK_V', 'WK4_WS3', 'WK5_V1', 'WK5_V2', 'WK5_V3',\n",
       "       'WK5_WS4', 'WK5_WS5', 'WK5_Master3', 'WK6_WS6', 'WK6_SUK_V', 'WK7_V1',\n",
       "       'WK7_WS7', 'WK9_V1', 'WK9_V2', 'WK9_WS8', 'WK9_Master4', 'WK10_V1',\n",
       "       'WK10_V2', 'WK10_V3', 'WK10_V4', 'WK10_WS9', 'WK10_Master5',\n",
       "       'WK10_WS10', 'WK10_SUK_V', 'Pre_Recorded_Total_Hours',\n",
       "       'SUK_Total_Hours', 'Masterclass_Total_Hours', 'Workshop_Total_Hours',\n",
       "       'Program_Total_Hours', 'Pre_Recorded_Percentage', 'SUK_Percentage',\n",
       "       'Masterclass_Percentage', 'Workshop_Percentage', 'Program_Percentage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9689969a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Email',\n",
       " 'Video ! : Why women in STEM. What has Kalpana program achieved until now? - 62c7d78d0cf273488220b8d4',\n",
       " 'Video @ : What do we expect from our Kalpana fellows? - 62c7d10e0cf273488220b5bf',\n",
       " 'SUK + : Program Inauguration & Icebreaker 7 Sept - 66dc3e605ae308684ff5e726',\n",
       " 'Video # : Goal setting - Part 1 - 6529ffc5e4b0a564951cb119',\n",
       " 'Video $ : Goal setting - Part 2 - 6529ffd4e4b08a0cfb8d0628',\n",
       " 'Workshop @ : Setting Aspirational Goals, Thursday, Sept 12, 3-4 PM - 66de9bf79027bf0127e793ec',\n",
       " 'Master Class # : Searching and Securing Internships - Friday, Sept 13, 3-4 PM - 66e125d35e916d19ae3df9b1',\n",
       " 'SUK ! : Speak Up Kalpana session, Friday, Sept 20, 3-4 PM - 66e13e6e91e40e365df6fdd4',\n",
       " 'Video % : What is SMART goal ? - 62b3fded0cf2f5120a60c932',\n",
       " 'Video ^ : What is the difference between a goal and a SMART goal ? - 62b3fe320cf2f5120a60c946',\n",
       " 'Workshop $ : Setting SMART Goals, Thursday, Sept 26, 3-4 PM - 66e13f3476aec348317d2f82',\n",
       " 'Master Class $ : Acing your CV and Resume - Friday, Sept 27, 3-4 PM - 669bcf052dc20b47fbf2fe99',\n",
       " 'SUK @ : Speak Up Kalpana session, Friday, Oct 04, 3-4 PM - 66e16961350fd25781854113',\n",
       " 'Workshop !! : Resume Building, Thursday, Oct 03, 3-4 PM - 66fe60ed848a814842ca4817',\n",
       " 'Video $? : What is Career Action Planner (CAP)? - 66adf521860b71150ba38ee7',\n",
       " 'Video %+ : CAP -Setting aspirational goals - 66adf537860b71150ba38ef0',\n",
       " 'Video %! : CAP - Common dangers of goal setting - 66adf54c323eaf7ca90260c2',\n",
       " 'Workshop !# : Resume Part 2 - Tuesday, Oct 08, 3-4 PM - 66e1292a0b2ff80834043181',\n",
       " 'Workshop % : CAP - Goal A and Goal B, Thursday, Oct 10, 3-4 PM - 66e157bb9764e21dc82fefe1',\n",
       " 'Master Class ! : Career Exploration -Friday, Oct 11, 3-4 PM - 66e16af647cfba543f5bfc02',\n",
       " 'Workshop ^ : Career Exploration, Thursday, Oct 17, 3-4 PM - 66e1590a07ac5d223c16704a',\n",
       " 'SUK # : Speak Up Kalpana session, Friday, Oct 18, 3-4 PM - 66e16a2a7786466b28e20d98',\n",
       " 'Video %@ : CAP - Career exploration (Finding key employers, career options and skills to learn) - 66ca11aabdcd457fa70393bf',\n",
       " 'Workshop & : CAP - Careers, Thursday, Oct 24, 3-4 PM - 66e15985ed86491af380d907',\n",
       " 'Video %# : Importance of milestones - Setting milestones & Adjustments - 66d1cce5e7eb841494dbdcbc',\n",
       " 'Video %$ : CAP Overview & How to use after Kalpana - 66d1cdd362a6586f5d9d8487',\n",
       " 'Workshop * : CAP - Milestones, Thursday, Nov 07, 3-4 PM - 672f58e966985879c05686cd',\n",
       " 'Master Class % : Making the most of LinkedIn - Friday, Nov 08, 3-4 PM - 6726f73b9b5cf20d990b91ce',\n",
       " 'Video & : What is SWOT & its importance - 65dd86dbe4b06c493da95be5',\n",
       " 'Video * : How to do SWOT analysis? - 65dd86f7e4b0b6b63afd1fa1',\n",
       " 'Video ? : What are Strengths, Weakness, Opportunities, Threats - 65dd86ede4b05b3f96b721a8',\n",
       " 'Video !+ : Deep dive into SWOT template with example - 62c56bfa0cf2d3022e87e6c5',\n",
       " 'Workshop ? : SWOT, Thursday, Nov 14, 3-4 PM - 66e15c6c589f9621d8a97795',\n",
       " 'Master Class !^ : Job Interview Preparation, Friday, Nov 15, 3-4 PM - 66e16c0a47cfba543f5bfd06',\n",
       " 'Workshop !@ : Securing Internships & LinkedIn, Nov 21, 3-4 PM - 66e15ea30e23e702f79f9df6',\n",
       " 'SUK $ : Program Finale - 66e16db64147c71f5908246d']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a68cbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "oldcol = [col for col in oldcol if not col.startswith(\"Week\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1b7a2173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Email',\n",
       " 'Video ! : Why women in STEM. What has Kalpana program achieved until now? - 62c7d78d0cf273488220b8d4',\n",
       " 'Video @ : What do we expect from our Kalpana fellows? - 62c7d10e0cf273488220b5bf',\n",
       " 'SUK + : Program Inauguration & Icebreaker 7 Sept - 66dc3e605ae308684ff5e726',\n",
       " 'Video # : Goal setting - Part 1 - 6529ffc5e4b0a564951cb119',\n",
       " 'Video $ : Goal setting - Part 2 - 6529ffd4e4b08a0cfb8d0628',\n",
       " 'Workshop @ : Setting Aspirational Goals, Thursday, Sept 12, 3-4 PM - 66de9bf79027bf0127e793ec',\n",
       " 'Master Class # : Searching and Securing Internships - Friday, Sept 13, 3-4 PM - 66e125d35e916d19ae3df9b1',\n",
       " 'SUK ! : Speak Up Kalpana session, Friday, Sept 20, 3-4 PM - 66e13e6e91e40e365df6fdd4',\n",
       " 'Video % : What is SMART goal ? - 62b3fded0cf2f5120a60c932',\n",
       " 'Video ^ : What is the difference between a goal and a SMART goal ? - 62b3fe320cf2f5120a60c946',\n",
       " 'Workshop $ : Setting SMART Goals, Thursday, Sept 26, 3-4 PM - 66e13f3476aec348317d2f82',\n",
       " 'Master Class $ : Acing your CV and Resume - Friday, Sept 27, 3-4 PM - 669bcf052dc20b47fbf2fe99',\n",
       " 'SUK @ : Speak Up Kalpana session, Friday, Oct 04, 3-4 PM - 66e16961350fd25781854113',\n",
       " 'Workshop !! : Resume Building, Thursday, Oct 03, 3-4 PM - 66fe60ed848a814842ca4817',\n",
       " 'Video $? : What is Career Action Planner (CAP)? - 66adf521860b71150ba38ee7',\n",
       " 'Video %+ : CAP -Setting aspirational goals - 66adf537860b71150ba38ef0',\n",
       " 'Video %! : CAP - Common dangers of goal setting - 66adf54c323eaf7ca90260c2',\n",
       " 'Workshop !# : Resume Part 2 - Tuesday, Oct 08, 3-4 PM - 66e1292a0b2ff80834043181',\n",
       " 'Workshop % : CAP - Goal A and Goal B, Thursday, Oct 10, 3-4 PM - 66e157bb9764e21dc82fefe1',\n",
       " 'Master Class ! : Career Exploration -Friday, Oct 11, 3-4 PM - 66e16af647cfba543f5bfc02',\n",
       " 'Workshop ^ : Career Exploration, Thursday, Oct 17, 3-4 PM - 66e1590a07ac5d223c16704a',\n",
       " 'SUK # : Speak Up Kalpana session, Friday, Oct 18, 3-4 PM - 66e16a2a7786466b28e20d98',\n",
       " 'Video %@ : CAP - Career exploration (Finding key employers, career options and skills to learn) - 66ca11aabdcd457fa70393bf',\n",
       " 'Workshop & : CAP - Careers, Thursday, Oct 24, 3-4 PM - 66e15985ed86491af380d907',\n",
       " 'Video %# : Importance of milestones - Setting milestones & Adjustments - 66d1cce5e7eb841494dbdcbc',\n",
       " 'Video %$ : CAP Overview & How to use after Kalpana - 66d1cdd362a6586f5d9d8487',\n",
       " 'Workshop * : CAP - Milestones, Thursday, Nov 07, 3-4 PM - 672f58e966985879c05686cd',\n",
       " 'Master Class % : Making the most of LinkedIn - Friday, Nov 08, 3-4 PM - 6726f73b9b5cf20d990b91ce',\n",
       " 'Video & : What is SWOT & its importance - 65dd86dbe4b06c493da95be5',\n",
       " 'Video * : How to do SWOT analysis? - 65dd86f7e4b0b6b63afd1fa1',\n",
       " 'Video ? : What are Strengths, Weakness, Opportunities, Threats - 65dd86ede4b05b3f96b721a8',\n",
       " 'Video !+ : Deep dive into SWOT template with example - 62c56bfa0cf2d3022e87e6c5',\n",
       " 'Workshop ? : SWOT, Thursday, Nov 14, 3-4 PM - 66e15c6c589f9621d8a97795',\n",
       " 'Master Class !^ : Job Interview Preparation, Friday, Nov 15, 3-4 PM - 66e16c0a47cfba543f5bfd06',\n",
       " 'Workshop !@ : Securing Internships & LinkedIn, Nov 21, 3-4 PM - 66e15ea30e23e702f79f9df6',\n",
       " 'SUK $ : Program Finale - 66e16db64147c71f5908246d']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "44870a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "#data.columns = oldcol[:len(data.columns)]\n",
    "# Now, let's say you want to change the column names of 'old_dataset' based on 'filtered_column_names'\n",
    "num_columns_to_rename = min(len(merged_df.columns), len(oldcol))\n",
    "print(num_columns_to_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "04ccc2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns=dict(zip(merged_df.columns[:num_columns_to_rename], oldcol[:num_columns_to_rename])), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d960fb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email',\n",
       "       'Video ! : Why women in STEM. What has Kalpana program achieved until now? - 62c7d78d0cf273488220b8d4',\n",
       "       'Video @ : What do we expect from our Kalpana fellows? - 62c7d10e0cf273488220b5bf',\n",
       "       'SUK + : Program Inauguration & Icebreaker 7 Sept - 66dc3e605ae308684ff5e726',\n",
       "       'Video # : Goal setting - Part 1 - 6529ffc5e4b0a564951cb119',\n",
       "       'Video $ : Goal setting - Part 2 - 6529ffd4e4b08a0cfb8d0628',\n",
       "       'Workshop @ : Setting Aspirational Goals, Thursday, Sept 12, 3-4 PM - 66de9bf79027bf0127e793ec',\n",
       "       'Master Class # : Searching and Securing Internships - Friday, Sept 13, 3-4 PM - 66e125d35e916d19ae3df9b1',\n",
       "       'SUK ! : Speak Up Kalpana session, Friday, Sept 20, 3-4 PM - 66e13e6e91e40e365df6fdd4',\n",
       "       'Video % : What is SMART goal ? - 62b3fded0cf2f5120a60c932',\n",
       "       'Video ^ : What is the difference between a goal and a SMART goal ? - 62b3fe320cf2f5120a60c946',\n",
       "       'Workshop $ : Setting SMART Goals, Thursday, Sept 26, 3-4 PM - 66e13f3476aec348317d2f82',\n",
       "       'Master Class $ : Acing your CV and Resume - Friday, Sept 27, 3-4 PM - 669bcf052dc20b47fbf2fe99',\n",
       "       'SUK @ : Speak Up Kalpana session, Friday, Oct 04, 3-4 PM - 66e16961350fd25781854113',\n",
       "       'Workshop !! : Resume Building, Thursday, Oct 03, 3-4 PM - 66fe60ed848a814842ca4817',\n",
       "       'Video $? : What is Career Action Planner (CAP)? - 66adf521860b71150ba38ee7',\n",
       "       'Video %+ : CAP -Setting aspirational goals - 66adf537860b71150ba38ef0',\n",
       "       'Video %! : CAP - Common dangers of goal setting - 66adf54c323eaf7ca90260c2',\n",
       "       'Workshop !# : Resume Part 2 - Tuesday, Oct 08, 3-4 PM - 66e1292a0b2ff80834043181',\n",
       "       'Workshop % : CAP - Goal A and Goal B, Thursday, Oct 10, 3-4 PM - 66e157bb9764e21dc82fefe1',\n",
       "       'Master Class ! : Career Exploration -Friday, Oct 11, 3-4 PM - 66e16af647cfba543f5bfc02',\n",
       "       'Workshop ^ : Career Exploration, Thursday, Oct 17, 3-4 PM - 66e1590a07ac5d223c16704a',\n",
       "       'SUK # : Speak Up Kalpana session, Friday, Oct 18, 3-4 PM - 66e16a2a7786466b28e20d98',\n",
       "       'Video %@ : CAP - Career exploration (Finding key employers, career options and skills to learn) - 66ca11aabdcd457fa70393bf',\n",
       "       'Workshop & : CAP - Careers, Thursday, Oct 24, 3-4 PM - 66e15985ed86491af380d907',\n",
       "       'Video %# : Importance of milestones - Setting milestones & Adjustments - 66d1cce5e7eb841494dbdcbc',\n",
       "       'Video %$ : CAP Overview & How to use after Kalpana - 66d1cdd362a6586f5d9d8487',\n",
       "       'Workshop * : CAP - Milestones, Thursday, Nov 07, 3-4 PM - 672f58e966985879c05686cd',\n",
       "       'Master Class % : Making the most of LinkedIn - Friday, Nov 08, 3-4 PM - 6726f73b9b5cf20d990b91ce',\n",
       "       'Video & : What is SWOT & its importance - 65dd86dbe4b06c493da95be5',\n",
       "       'Video * : How to do SWOT analysis? - 65dd86f7e4b0b6b63afd1fa1',\n",
       "       'Video ? : What are Strengths, Weakness, Opportunities, Threats - 65dd86ede4b05b3f96b721a8',\n",
       "       'Video !+ : Deep dive into SWOT template with example - 62c56bfa0cf2d3022e87e6c5',\n",
       "       'Workshop ? : SWOT, Thursday, Nov 14, 3-4 PM - 66e15c6c589f9621d8a97795',\n",
       "       'Master Class !^ : Job Interview Preparation, Friday, Nov 15, 3-4 PM - 66e16c0a47cfba543f5bfd06',\n",
       "       'Workshop !@ : Securing Internships & LinkedIn, Nov 21, 3-4 PM - 66e15ea30e23e702f79f9df6',\n",
       "       'SUK $ : Program Finale - 66e16db64147c71f5908246d',\n",
       "       'Pre_Recorded_Total_Hours', 'SUK_Total_Hours',\n",
       "       'Masterclass_Total_Hours', 'Workshop_Total_Hours',\n",
       "       'Program_Total_Hours', 'Pre_Recorded_Percentage', 'SUK_Percentage',\n",
       "       'Masterclass_Percentage', 'Workshop_Percentage', 'Program_Percentage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ce10e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the columns by ':' and keep only the first part\n",
    "merged_df.columns = merged_df.columns.str.split (':').str [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e19babfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'Video ! ', 'Video @ ', 'SUK + ', 'Video # ', 'Video $ ',\n",
       "       'Workshop @ ', 'Master Class # ', 'SUK ! ', 'Video % ', 'Video ^ ',\n",
       "       'Workshop $ ', 'Master Class $ ', 'SUK @ ', 'Workshop !! ', 'Video $? ',\n",
       "       'Video %+ ', 'Video %! ', 'Workshop !# ', 'Workshop % ',\n",
       "       'Master Class ! ', 'Workshop ^ ', 'SUK # ', 'Video %@ ', 'Workshop & ',\n",
       "       'Video %# ', 'Video %$ ', 'Workshop * ', 'Master Class % ', 'Video & ',\n",
       "       'Video * ', 'Video ? ', 'Video !+ ', 'Workshop ? ', 'Master Class !^ ',\n",
       "       'Workshop !@ ', 'SUK $ ', 'Pre_Recorded_Total_Hours', 'SUK_Total_Hours',\n",
       "       'Masterclass_Total_Hours', 'Workshop_Total_Hours',\n",
       "       'Program_Total_Hours', 'Pre_Recorded_Percentage', 'SUK_Percentage',\n",
       "       'Masterclass_Percentage', 'Workshop_Percentage', 'Program_Percentage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6627f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns = merged_df.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8938f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_column(col):\n",
    "    # Split the column name by space\n",
    "    parts = col.split()\n",
    "    special_chars = {'!': '1', '@': '2', '#': '3', '$': '4', '%': '5', '^': '6', '&': '7', '*': '8', '?': '9', '+': '0'}\n",
    "    \n",
    "    # If the first part is 'SUK', add the number corresponding to the last character\n",
    "    if parts[0] == 'SUK':\n",
    "        return parts[0] + special_chars.get(parts[1][-1], '0')\n",
    "    \n",
    "    # If the first part is 'Video', use the prefix 'VID' and the numbers corresponding to the last two characters\n",
    "    elif parts[0] == 'Video':\n",
    "        last_char = parts[1][-1]\n",
    "        second_last_char = parts[1][-2] if len(parts[1]) > 1 else '+'\n",
    "        return 'VID' + special_chars.get(second_last_char, '0') + special_chars.get(last_char, '0')\n",
    "\n",
    "    # If the first part is 'Workshop', use the prefix 'VID' and the numbers corresponding to the last two characters\n",
    "    elif parts[0] == 'Workshop':\n",
    "        last_char = parts[1][-1]\n",
    "        second_last_char = parts[1][-2] if len(parts[1]) > 1 else '+'\n",
    "        return 'WS' + special_chars.get(second_last_char, '0') + special_chars.get(last_char, '0')\n",
    "    \n",
    "    # If the first part contains 'Master' and 'Class' is present, use the prefix 'MC' and the numbers corresponding to the last two characters\n",
    "    elif 'Master' in parts and 'Class' in parts:\n",
    "        index = parts.index('Master')\n",
    "        last_char = parts[index + 2][-1]\n",
    "        second_last_char = parts[index + 2][-2] if len(parts[index + 2]) > 1 else '+'\n",
    "        return 'MC' + special_chars.get(second_last_char, '0') + special_chars.get(last_char, '0')\n",
    "    \n",
    "    # Otherwise, return the column name as it is\n",
    "    else:\n",
    "        return col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b37d79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column names using the rename_column function\n",
    "merged_df.rename(columns=rename_column, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4a2c6fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'VID01', 'VID02', 'SUK0', 'VID03', 'VID04', 'WS02', 'MC03',\n",
       "       'SUK1', 'VID05', 'VID06', 'WS04', 'MC04', 'SUK2', 'WS11', 'VID49',\n",
       "       'VID50', 'VID51', 'WS13', 'WS05', 'MC01', 'WS06', 'SUK3', 'VID52',\n",
       "       'WS07', 'VID53', 'VID54', 'WS08', 'MC05', 'VID07', 'VID08', 'VID09',\n",
       "       'VID10', 'WS09', 'MC16', 'WS12', 'SUK4', 'Pre_Recorded_Total_Hours',\n",
       "       'SUK_Total_Hours', 'Masterclass_Total_Hours', 'Workshop_Total_Hours',\n",
       "       'Program_Total_Hours', 'Pre_Recorded_Percentage', 'SUK_Percentage',\n",
       "       'Masterclass_Percentage', 'Workshop_Percentage', 'Program_Percentage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dd236be9-5a8f-4b6e-8825-17530e567cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(639, 47)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "40edf405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email</th>\n",
       "      <th>VID01</th>\n",
       "      <th>VID02</th>\n",
       "      <th>SUK0</th>\n",
       "      <th>VID03</th>\n",
       "      <th>VID04</th>\n",
       "      <th>WS02</th>\n",
       "      <th>MC03</th>\n",
       "      <th>SUK1</th>\n",
       "      <th>VID05</th>\n",
       "      <th>VID06</th>\n",
       "      <th>WS04</th>\n",
       "      <th>MC04</th>\n",
       "      <th>SUK2</th>\n",
       "      <th>WS11</th>\n",
       "      <th>VID49</th>\n",
       "      <th>VID50</th>\n",
       "      <th>VID51</th>\n",
       "      <th>WS13</th>\n",
       "      <th>WS05</th>\n",
       "      <th>MC01</th>\n",
       "      <th>WS06</th>\n",
       "      <th>SUK3</th>\n",
       "      <th>VID52</th>\n",
       "      <th>WS07</th>\n",
       "      <th>VID53</th>\n",
       "      <th>VID54</th>\n",
       "      <th>WS08</th>\n",
       "      <th>MC05</th>\n",
       "      <th>VID07</th>\n",
       "      <th>VID08</th>\n",
       "      <th>VID09</th>\n",
       "      <th>VID10</th>\n",
       "      <th>WS09</th>\n",
       "      <th>MC16</th>\n",
       "      <th>WS12</th>\n",
       "      <th>SUK4</th>\n",
       "      <th>Pre_Recorded_Total_Hours</th>\n",
       "      <th>SUK_Total_Hours</th>\n",
       "      <th>Masterclass_Total_Hours</th>\n",
       "      <th>Workshop_Total_Hours</th>\n",
       "      <th>Program_Total_Hours</th>\n",
       "      <th>Pre_Recorded_Percentage</th>\n",
       "      <th>SUK_Percentage</th>\n",
       "      <th>Masterclass_Percentage</th>\n",
       "      <th>Workshop_Percentage</th>\n",
       "      <th>Program_Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>monu1288@gmail.com</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sreejith.sreenivasan@vigyanshaala.com</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.22</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spjaybhaye01@gmail.com</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>salini.1405@gmail.com</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ds9284751@gmail.com</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.01</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.43</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.78</td>\n",
       "      <td>9.18</td>\n",
       "      <td>7.87</td>\n",
       "      <td>10.36</td>\n",
       "      <td>29.19</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>90.93</td>\n",
       "      <td>91.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Email  VID01  VID02  SUK0  VID03  VID04  \\\n",
       "0                     monu1288@gmail.com   0.00   0.00  0.00   0.00   0.00   \n",
       "1  sreejith.sreenivasan@vigyanshaala.com   0.00   0.00  0.05   0.03   0.00   \n",
       "2                 spjaybhaye01@gmail.com   0.00   0.00  0.00   0.00   0.00   \n",
       "3                  salini.1405@gmail.com   0.00   0.00  0.02   0.00   0.00   \n",
       "4                    ds9284751@gmail.com   0.08   0.08  2.88   0.08   0.11   \n",
       "\n",
       "   WS02  MC03  SUK1  VID05  VID06  WS04  MC04  SUK2  WS11  VID49  VID50  \\\n",
       "0  0.08  0.00  0.00   0.00   0.00   0.0  0.00  0.00  0.00   0.00   0.00   \n",
       "1  0.00  0.00  0.00   0.00   0.00   0.0  0.00  0.00  0.00   0.00   0.00   \n",
       "2  0.00  0.00  0.00   0.00   0.00   0.0  0.00  0.00  0.00   0.00   0.00   \n",
       "3  0.00  0.00  0.00   0.00   0.00   0.0  0.00  0.00  0.00   0.00   0.00   \n",
       "4  1.01  2.68  2.29   0.24   0.08   1.2  2.43  1.99  1.12   0.05   0.08   \n",
       "\n",
       "   VID51  WS13  WS05  MC01  WS06  SUK3  VID52  WS07  VID53  VID54  WS08  MC05  \\\n",
       "0   0.00  0.00  0.00  0.00   0.0  0.00   0.00  0.00   0.00    0.0  0.01  0.00   \n",
       "1   0.00  0.00  0.00  0.00   0.0  0.00   0.00  0.00   0.00    0.0  0.00  0.00   \n",
       "2   0.00  0.00  0.00  0.00   0.0  0.00   0.00  0.00   0.00    0.0  0.00  0.00   \n",
       "3   0.00  0.00  0.00  0.00   0.0  0.00   0.00  0.00   0.00    0.0  0.00  0.00   \n",
       "4   0.16  1.23  1.62  1.64   1.0  2.03   0.19  1.15   0.27    0.1  0.18  1.12   \n",
       "\n",
       "   VID07  VID08  VID09  VID10  WS09  MC16  WS12  SUK4  \\\n",
       "0   0.00    0.0   0.00    0.0  0.00   0.0   0.0   0.0   \n",
       "1   0.00    0.0   0.00    0.0  0.00   0.0   0.0   0.0   \n",
       "2   0.00    0.0   0.00    0.0  0.00   0.0   0.0   0.0   \n",
       "3   0.00    0.0   0.00    0.0  0.00   0.0   0.0   0.0   \n",
       "4   0.04    0.1   0.13    0.0  1.85   0.0   0.0   0.0   \n",
       "\n",
       "   Pre_Recorded_Total_Hours  SUK_Total_Hours  Masterclass_Total_Hours  \\\n",
       "0                      0.00             0.00                     0.00   \n",
       "1                      0.04             0.05                     0.00   \n",
       "2                      0.00             0.00                     0.00   \n",
       "3                      0.00             0.02                     0.00   \n",
       "4                      1.78             9.18                     7.87   \n",
       "\n",
       "   Workshop_Total_Hours  Program_Total_Hours  Pre_Recorded_Percentage  \\\n",
       "0                  0.09                 0.09                     0.00   \n",
       "1                  0.00                 0.08                     5.22   \n",
       "2                  0.00                 0.00                     0.00   \n",
       "3                  0.00                 0.02                     0.16   \n",
       "4                 10.36                29.19                    93.75   \n",
       "\n",
       "   SUK_Percentage  Masterclass_Percentage  Workshop_Percentage  \\\n",
       "0            0.02                    0.03                 0.97   \n",
       "1            1.18                    0.00                 0.00   \n",
       "2            0.00                    0.00                 0.00   \n",
       "3            0.39                    0.00                 0.00   \n",
       "4          100.00                   80.00                90.93   \n",
       "\n",
       "   Program_Percentage  \n",
       "0                0.25  \n",
       "1                1.60  \n",
       "2                0.00  \n",
       "3                0.14  \n",
       "4               91.17  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a002ea26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "mask = merged_df.columns.duplicated().sum()\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97664d2-2f7c-4aff-a0d9-9b36f9d4d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting data to Output files\n",
    "merged_df.to_csv(r\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb154fd",
   "metadata": {},
   "source": [
    "# Storing data on MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection\n",
    "conn= msql.connect(host='',user='',password=\"\",database=\"\",auth_plugin='')\n",
    "cursor =conn.cursor() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "24cbb91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the existing columns in the database\n",
    "cursor.execute(\"SHOW COLUMNS FROM 08_incubator_and_attendance_monitoring\")\n",
    "existing_columns = [col[0] for col in cursor.fetchall()]\n",
    "\n",
    "# Define the column name before which the new column should be added\n",
    "target_column = 'Pre_Recorded_Total_Hours'\n",
    "\n",
    "# Check if any new columns exist in the dataframe but not in the database\n",
    "new_columns = [col for col in merged_df.columns if col not in existing_columns]\n",
    "if new_columns:\n",
    "    # Add new columns to the database before the target column\n",
    "    for col in reversed(new_columns):\n",
    "        if col not in existing_columns:\n",
    "            # Get the index of the target column\n",
    "            target_column_index = existing_columns.index(target_column)\n",
    "            # Set the data type based on whether the column name starts with Comment\n",
    "            data_type = \"varchar(50)\" \n",
    "            alter_query = f\"ALTER TABLE 08_incubator_and_attendance_monitoring ADD COLUMN {col} {data_type} AFTER {existing_columns[target_column_index - 1]}\"\n",
    "            cursor.execute(alter_query)\n",
    "            existing_columns.insert(target_column_index - 1, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c532992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing code for inserting data into the database table\n",
    "for i, row in merged_df.iterrows():\n",
    "    row = [None if isinstance(val, float) and math.isnan(val) else val for val in row] # replace \"nan\" values with None\n",
    "    columns = ','.join(merged_df.columns)\n",
    "    placeholders = ','.join(['%s']*len(row))\n",
    "    # Construct the INSERT query with ON DUPLICATE KEY UPDATE clause\n",
    "    query = f\"INSERT INTO 08_incubator_and_attendance_monitoring ({columns}) VALUES ({placeholders}) ON DUPLICATE KEY UPDATE \"\n",
    "    query += \", \".join([f\"{col}=VALUES({col})\" for col in merged_df.columns if col != 'Email'])\n",
    "    # Execute the query\n",
    "    cursor.execute(query, tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6764e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit the transaction\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "63390d1c-412b-4bf9-ac95-a95cf450eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a38e79-3c5a-484a-adbf-18a44abf5940",
   "metadata": {},
   "source": [
    "# Storing Data in Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadb4bd-3ff3-4d97-8bc5-f0e68f670c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supabase URL and API key\n",
    "url = ''\n",
    "api_key = ''\n",
    "\n",
    "\n",
    "# Headers for the request\n",
    "headers = {\n",
    "    'apikey': api_key,\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "    'Content-Type': 'application/json',\n",
    "    'Prefer': 'resolution=merge-duplicates'  # Enable upsert functionality\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af990458-fc6c-4ceb-882b-5cf45d44599c",
   "metadata": {},
   "source": [
    "### 03_enroll_date Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "91fb954f-d66a-4026-8d6d-3fc267fbdc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column 'old_column_name' to 'new_column_name'\n",
    "Enroll.rename(columns={'Enroll Date': 'Incubator'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ff7672fe-5830-4535-89a1-6fa50989f619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Email', 'Incubator'], dtype='object')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Enroll.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e6b1dc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final batch of 503 rows upserted successfully\n"
     ]
    }
   ],
   "source": [
    "table_name = '03_enroll_date'\n",
    "\n",
    "# Batch size for upserting\n",
    "batch_size = 1000  # You can adjust this value based on your performance needs\n",
    "\n",
    "# List to store rows before sending them in batches\n",
    "batch_data = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for i, row in Enroll.iterrows():\n",
    "    # Replace NaN values with None\n",
    "    row = [None if isinstance(val, float) and math.isnan(val) else val for val in row]\n",
    "    # Convert row to a dictionary\n",
    "    row_dict = dict(zip(Enroll.columns, row))\n",
    "    \n",
    "    # Add the row to the batch\n",
    "    batch_data.append(row_dict)\n",
    "    \n",
    "    # If the batch size is reached, send the data\n",
    "    if len(batch_data) >= batch_size:\n",
    "        # Send a batch of rows\n",
    "        response = requests.post(f'{url}/rest/v1/{table_name}', headers=headers, json=batch_data)\n",
    "        \n",
    "        # Check response\n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f'Batch of {len(batch_data)} rows upserted successfully')\n",
    "        else:\n",
    "            print(f'Failed to upsert batch: {response.status_code}, {response.text}')\n",
    "        \n",
    "        # Clear the batch after sending\n",
    "        batch_data = []\n",
    "\n",
    "# Send any remaining rows in the last batch\n",
    "if batch_data:\n",
    "    response = requests.post(f'{url}/rest/v1/{table_name}', headers=headers, json=batch_data)\n",
    "    \n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f'Final batch of {len(batch_data)} rows upserted successfully')\n",
    "    else:\n",
    "        print(f'Failed to upsert final batch: {response.status_code}, {response.text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295aa7ef-4ab0-45a1-81d3-f1229fa01088",
   "metadata": {},
   "source": [
    "### 02_payment_details Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "800fe42e-79b6-48fa-a503-6f94837c4919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column 'old_column_name' to 'new_column_name'\n",
    "Payment.rename(columns={'Assigned Through': 'Incubator_Fee'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "66b33288-1e65-4e3e-b9e5-3624cff76898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final batch of 503 rows upserted successfully\n"
     ]
    }
   ],
   "source": [
    "table_name = '02_payment_details'\n",
    "\n",
    "# Batch size for upserting\n",
    "batch_size = 1000  # You can adjust this value based on your performance needs\n",
    "\n",
    "# List to store rows before sending them in batches\n",
    "batch_data = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for i, row in Payment.iterrows():\n",
    "    # Replace NaN values with None\n",
    "    row = [None if isinstance(val, float) and math.isnan(val) else val for val in row]\n",
    "    # Convert row to a dictionary\n",
    "    row_dict = dict(zip(Payment.columns, row))\n",
    "    \n",
    "    # Add the row to the batch\n",
    "    batch_data.append(row_dict)\n",
    "    \n",
    "    # If the batch size is reached, send the data\n",
    "    if len(batch_data) >= batch_size:\n",
    "        # Send a batch of rows\n",
    "        response = requests.post(f'{url}/rest/v1/{table_name}', headers=headers, json=batch_data)\n",
    "        \n",
    "        # Check response\n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f'Batch of {len(batch_data)} rows upserted successfully')\n",
    "        else:\n",
    "            print(f'Failed to upsert batch: {response.status_code}, {response.text}')\n",
    "        \n",
    "        # Clear the batch after sending\n",
    "        batch_data = []\n",
    "\n",
    "# Send any remaining rows in the last batch\n",
    "if batch_data:\n",
    "    response = requests.post(f'{url}/rest/v1/{table_name}', headers=headers, json=batch_data)\n",
    "    \n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f'Final batch of {len(batch_data)} rows upserted successfully')\n",
    "    else:\n",
    "        print(f'Failed to upsert final batch: {response.status_code}, {response.text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e77cb-9185-4ce2-bff7-bd34df1cb4e8",
   "metadata": {},
   "source": [
    "### 15_batch_lastlogin_startdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fe6704fb-522e-4ac2-bbcd-f588f751f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column 'old_column_name' to 'new_column_name'\n",
    "Batch.rename(columns={'Last Login': 'Incubator_Last_Login'}, inplace=True)\n",
    "Batch.rename(columns={'Start Date': 'Incubator_Last_Login'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b5eb43fa-988f-4dad-b03b-42a58ed18ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final batch of 503 rows upserted successfully\n"
     ]
    }
   ],
   "source": [
    "table_name = '15_batch_lastlogin_startdate'\n",
    "\n",
    "# Batch size for upserting\n",
    "batch_size = 1000  # You can adjust this value based on your performance needs\n",
    "\n",
    "# List to store rows before sending them in batches\n",
    "batch_data = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for i, row in Batch.iterrows():\n",
    "    # Replace NaN values with None\n",
    "    row = [None if isinstance(val, float) and math.isnan(val) else val for val in row]\n",
    "    # Convert row to a dictionary\n",
    "    row_dict = dict(zip(Batch.columns, row))\n",
    "    \n",
    "    # Add the row to the batch\n",
    "    batch_data.append(row_dict)\n",
    "    \n",
    "    # If the batch size is reached, send the data\n",
    "    if len(batch_data) >= batch_size:\n",
    "        # Send a batch of rows\n",
    "        response = requests.post(f'{url}/rest/v1/{table_name}', headers=headers, json=batch_data)\n",
    "        \n",
    "        # Check response\n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f'Batch of {len(batch_data)} rows upserted successfully')\n",
    "        else:\n",
    "            print(f'Failed to upsert batch: {response.status_code}, {response.text}')\n",
    "        \n",
    "        # Clear the batch after sending\n",
    "        batch_data = []\n",
    "\n",
    "# Send any remaining rows in the last batch\n",
    "if batch_data:\n",
    "    response = requests.post(f'{url}/rest/v1/{table_name}', headers=headers, json=batch_data)\n",
    "    \n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f'Final batch of {len(batch_data)} rows upserted successfully')\n",
    "    else:\n",
    "        print(f'Failed to upsert final batch: {response.status_code}, {response.text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc1b5d-b134-4313-900b-52eb1b8d07c7",
   "metadata": {},
   "source": [
    "### 08_incubator_and_attendance_monitoring Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "eefb18dd-2b18-41ad-bcdb-33bb105d7b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final batch of 639 rows upserted successfully\n"
     ]
    }
   ],
   "source": [
    "table_name = '08_incubator_and_attendance_monitoring'\n",
    "\n",
    "# Batch size for upserting\n",
    "batch_size = 1000  # You can adjust this value based on your performance needs\n",
    "\n",
    "# List to store rows before sending them in batches\n",
    "batch_data = []\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for i, row in merged_df.iterrows():\n",
    "    # Replace NaN values with None\n",
    "    row = [None if isinstance(val, float) and math.isnan(val) else val for val in row]\n",
    "    # Convert row to a dictionary\n",
    "    row_dict = dict(zip(merged_df.columns, row))\n",
    "    \n",
    "    # Add the row to the batch\n",
    "    batch_data.append(row_dict)\n",
    "    \n",
    "    # If the batch size is reached, send the data\n",
    "    if len(batch_data) >= batch_size:\n",
    "        # Send a batch of rows\n",
    "        response = requests.post(f'{url}/rest/v1/{table_name}', headers=headers, json=batch_data)\n",
    "        \n",
    "        # Check response\n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f'Batch of {len(batch_data)} rows upserted successfully')\n",
    "        else:\n",
    "            print(f'Failed to upsert batch: {response.status_code}, {response.text}')\n",
    "        \n",
    "        # Clear the batch after sending\n",
    "        batch_data = []\n",
    "\n",
    "# Send any remaining rows in the last batch\n",
    "if batch_data:\n",
    "    response = requests.post(f'{url}/rest/v1/{table_name}', headers=headers, json=batch_data)\n",
    "    \n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f'Final batch of {len(batch_data)} rows upserted successfully')\n",
    "    else:\n",
    "        print(f'Failed to upsert final batch: {response.status_code}, {response.text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70874d-f9d0-4431-b924-d97de4c091d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
